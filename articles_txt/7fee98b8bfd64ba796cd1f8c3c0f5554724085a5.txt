Mashable
She’ll be joining a number of organizations supporting the Campaign to Stop Killer Robots , whose primary objective is the pre-emptive ban on fully autonomous weapons . There are really only two outcomes on this issue — either the creation and spread of lethal autonomous weapons is banned or it isn’t. And by not banning them we’ll set a precedent that condones the moral and legal sovereignty of machines over our lives.
Once we approve the use of autonomous machines to kill without humans in the loop, we’re ceding what’s known as, “meaningful human control” for these systems. While the moral ramifications of war are certainly different than something like the manufacture of self-driving cars, once we’ve justified autonomous systems making life-and-death decisions, it’s easy to imagine our reliance on them for everything else.
“What we’re trying to do is demonstrate how the public feels about these issues,” noted Moon in our interview about her survey — which anyone can take — and upcoming meeting at the United Nations. “It’s something the UN has to consider when discussing the future of weapons systems at an international level.”
The Campaign to Stop Killer Robots launched in April of 2013 in response to the expanded use of unmanned armed vehicles that has proliferated over the past decade. Military technology has now expanded to the point where fully autonomous weapons that could choose and fire on targets without any human intervention are being developed by multiple countries around the world. As the campaign notes on their website , "Giving machines the power to decide who lives and dies on the battlefield is an unacceptable application of technology. Human control of any combat robot is essential to ensuring both humanitarian protection and effective legal control." Their solution calls for a preemptive prohibition of fully autonomous weapons that could be achieved via the negotiations and potential treaty being discussed in Geneva.
SEE ALSO: Robot firefighter shows off sea legs and puts out shipboard flames
Moon’s work with the Open Roboethics Initiative has set a strong precedent for the value of crowdsourcing opinions around the complex and deeply ethical issues surrounding Artificial Intelligence and robots. ORi’s site features multiple polls posing questions around scenarios regarding these issues to move beyond rhetoric and get people to more deeply consider how sentient machines will affect their lives. Consider, for instance, ORi’s polls, Should a Carebot Bring an Alcoholic a Drink? or, Would You Trust a Robot to Take Care of Your Grandma? These are the types of issues we’ll want to decide for ourselves when the time comes based on our personal ethics and background. While most of us are not equipped to make decisions surrounding military situations, we certainly can weigh in on the types of issues ORi presents today as a means to provide societal perspective on issues of machine autonomy.
“Where’s the data on what Koreans think about this issue (of lethal autonomous weapons), or people in Iran or Turkey? There’s a disconnect between decision makers and the public that’s something to keep in mind going ahead with Roboethics issues outside of the military discussion,” Moon said in our conversation. “We should engage the public to discuss the topic before it’s the next big thing to change the world.”
A question of responsibility
“We’ve been focusing on trying to define what an autonomous weapon is so it can be the basis for an international agreement,” explained Peter Asaro, Ph.D., an assistant professor in the School of Media Studies at The New School , as well as the co-founder and vice chair for the International Committee for Robot Arms Control (ICRAC), and a spokesperson for the Campaign to Stop Killer Robots . I interviewed him about his work and the concept of, “meaningful human control” for autonomous weapons. Rather than simply decry the use of any autonomous technology for military applications, Asaro’s objective is to ban the lethal nature of unmanned systems. “Things like deciding what and when to kill, these are not simply problems you can write an algorithm to solve. They’re really quite complicated and require reasoning that can’t be automated.”
A big part of meaningful human control has to do with accountability. Currently the notion of “mission autonomy” means these lethal systems could kill outside of the ability for humans to intervene. Unless there is a clear chain of accountability, culpability for kill decisions might fall to either the government requesting the systems, manufacturers producing them, or soldiers responsible for their deployment. This means technical malfunctions, far from rare in autonomous weaponry, could be the cause for a great deal of unjustified killing without clear notions of responsibility for the parties at fault. As Asaro notes, “I think it’s a moral criteria that a human makes the decision to take another human’s life.”
Like Moon and Asaro, I’m not advocating relinquishment regarding autonomous machines, which is the notion that society should stop research and development of AI. Besides the fact it would be impossible to enforce, that form of abstinence infers its own lack of accountability. By not embedding ethical principles at the design level for any autonomous machines, we cede responsibility for the moral quandaries future products might create.
In this sense, I see thought leaders like Ronald Arkin from Georgia Tech and his ideas about an ethical governor for autonomous weapons to be extremely important to the discussion around killer robots. While Arkin’s views are often cited to justify the use of lethal autonomous systems, he has often noted (as in this Robots Podcast ) that his work is about minimizing negative outcomes if the systems come into widespread use.
Clearpath Robotics in Ontario, Canada, has also set the bar for technology firms to weigh in authoritatively on the issue of lethal autonomous systems. In a press release from last August, Clearpath announced they were the first robotics company to join the Campaign to Stop Killer Robots . It’s a risky move as the firm supplies autonomous robots to multiple military clients in applications such as logistics and search and rescue. But as Ryan Gariepy, Co-Founder and CTO of Clearpath noted in the release, “In our eyes, no nation in the world is ready for killer robots — technologically, legally, or ethically.”  Going on to cite a common argument advocating the development of lethal autonomous weapons, Gariepy states:
Is a computer paired with the correct technology less likely to make rash, stress-driven decisions while under fire? Possibly. Conversely, would a robot have the morality, sense, or emotional understanding to intervene against orders that are wrong or inhumane? No. Would computers be able to make the kinds of subjective decisions required for checking the legitimacy of targets and ensuring the proportionate use of force in the foreseeable future? No. Could this technology lead those who possess it to value human life less? Quite frankly, we believe this will be the case.
The dignity of decision
Accountability around valuing human life is not something we should easily dismiss. The reason you should take the survey around this issue is that as humans we’re allowed some time to decide if we want to cede authority of life and death to machines. They say you can’t halt progress, but we can redefine it and we’re all accountable to clarify how autonomous technology could affect our lives. We can’t impose the onus of that decision on the military, government or AI manufacturers alone.
As Asaro brought up, there’s also a question of human dignity. “As a global civilization, if we’re willing to accept machines having the authority to take human life without any other human approving it I think it’s comparable to issues around slavery or torture. The reason from a moral perspective that we prohibit these things is not just that it harms an individual, but that it diminishes what it means to be human for everybody.”
©2005-2015 Mashable, Inc.

