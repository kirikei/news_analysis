UK opens probe into Facebook's psych experiment | Nation & World | The Seattle Times
LONDON --
British regulators are investigating revelations that Facebook treated hordes of its users like laboratory rats in an experiment probing into their emotions.
The Information Commissioner's Office said Wednesday that it wants to learn more about the circumstances underlying a 2-year-old study carried out by two U.S. universities and the world's largest social network.
The inquiry is being coordinated with authorities in Ireland, where Facebook has headquarters for its European operations, as well as with French regulators.
This is just the latest in a string of incidents that have raised questions about whether the privacy rights of Facebook's nearly 1.3 billion users are being trampled by the company's drive to dissect data and promote behavior that could help sell more online advertising.
In this case, Facebook allowed researchers to manipulate the content that appeared in the main section, or "news feed," of about 700,000 randomly selected users during a single week in January 2012. The data-scientists were trying to collect evidence to prove their thesis that people's moods could spread like an "emotional contagion" depending on the tenor of the content that they were reading.
The study concluded that people were more likely to post negative updates about their lives after the volume of positive information appearing in their Facebook feeds had been purposefully reduced by the researchers. The opposite reaction occurred when the number of negative posts appeared in people's news feeds.
None of the participants in the Facebook experiments were explicitly asked for their permission, though the social network's terms of use appears to allow for the company to manipulate what appears in users' news feeds however it sees fits.
Facebook's data-use policy says the Menlo Park, California, company can deploy user information for "internal operations, including troubleshooting, data analysis, testing, research and service improvement."
The reaction to the study itself provided evidence of how quickly an emotional contagion can spread online. The research was released a month ago, but it didn't provoke a backlash until the past few days after other social media sites and essays in The New York Times and The Atlantic raised red flags about the ethics of Facebook's experiment.
As it has done in several past breaches of privacy etiquette, Facebook is now trying to make amends.
Sheryl Sandberg, Facebook's chief operating officer, told television network NDTV in India that "we clearly communicated really badly about this and that we really regret." Later she added: "Facebook has apologized and certainly we never want to do anything that upsets users."
The words of contrition sounded hollow to Jeff Chester, executive director of the Center for Digital Democracy, a privacy-rights group. He points to Facebook job openings looking for researchers specializing in data mining and analysis as evidence that the company still has every intention of digging deeper into its users' psyches and preferences.
"They are engaged in secret surveillance of its users to figure out how to make more money for their advertisers," Chester said.
Whatever Facebook has been doing has been paying off for the company and its shareholders. Facebook's revenue last year rose 55 percent to $7.9 billion and its stock has nearly tripled in value during the past year.
The concern over Facebook's experiment comes amid interest in Europe about beefing up data-protection rules. The European Court of Justice last month ruled that Google must respond to users' requests seeking to remove links to personal information.
Suzy Moat, a Warwick Business School assistant professor of behavioral science, said businesses regularly do studies on how to influence behavior. She cited the example of Facebook and Amazon experimenting with showing different groups of people slightly different versions of their websites to see if one is better than another at getting customers to buy products.
"On the other hand, it's extremely understandable that many people are upset that their behavior may have been manipulated for purely scientific purposes without their consent," Moat said. "In particular, Facebook's user base is so wide that everyone wonders if they were in the experiment."
___
Liedtke reported from San Francisco. Mae Anderson in New York contributed to this report.

Facebook scientist defends emotions study
ROSE POWELL
Reuters
WHOOPS: Creator of study that tested users' emotions says he wanted to debunk idea that seeing positive posts makes you sad.
The outrage sparked by a recently revealed Facebook experiment on almost 700,000 users' emotions continues to burn, causing the research team to mount a public defence.
For a week in January 2012, researchers tweaked users' News Feeds to expose them to more positive or negative posts. They found a corresponding impact on what the affected users shared. In the paper they declare this proof of online emotional contagion.
The publication of the report prompted thousands of protest posts on social media, as well as concerns from leading academics and psychologists who have raised concerns about the ethics involved.
Facebook data scientist Adam Kramer, who coordinated the study, has now admitted the study may not be worth the uproar it caused.
"Having written and designed this experiment myself, I can tell you that our goal was never to upset anyone," Kramer wrote in a public Facebook post. "I can understand why some people have concerns about it, and my coauthors and I are very sorry for the way the paper described the research and any anxiety it caused. In hindsight, the research benefits of the paper may not have justified all of this anxiety."
Kramer claims Facebook and the two academics involved in the study, UCLA's Jamie Guillory and Cornell University's Jeffrey Hancock, conducted the emotion-altering experiment because they care about their users.
Kramer says he realises the team wasn't clear enough about their motivations in the report. The report states that they set out to test and debunk popular theories, such as that seeing positive posts from friends makes users feel worse and withdraw.
"The fact that people were more emotionally positive in response to positive emotion updates from their friends, stands in contrast to theories that suggest viewing positive posts by friends on Facebook may somehow affect us negatively, for example, via social comparison," wrote Kramer and co in the report.
In his Facebook post, Kramer celebrates the debunking of these theories. He claims they discovered the "exact opposite to what was then the conventional wisdom: Seeing a certain kind of emotion (positive) encourages it rather than suppresses it".
Despite this good news, users, psychologists and academics aren't celebrating. Many are raising concerns about the issue of consent, as users weren't directly informed about the study, prior, during or after the experiment.
Legally, Facebook users have already consented to an extensive privacy policy by having a profile. Facebook's data use policy allows this kind of experimentation with a clause explaining user data may be used for "internal operations, including troubleshooting, data analysis, testing, research and service improvement".
Under US federal law, research using human subjects requires those tested to give informed consent. This rule only applies to federally funded research and Facebook is a private company.
But James Grimmelmann, a professor of law at Maryland University, says the fact the paper was co-authored by two academics at universities that receive significant federal funding means these rules should apply, particularly because participants weren't all treated in an identical way.
"We wouldn't tell patients in a drug trial that the study was harmless because only a computer would ever know whether they received the placebo," wrote Grimmelmann in a blog post. "The unwitting participants in the Facebook study were told (seemingly by their friends) for a week either that the world was a dark and cheerless place or that it was a saccharine paradise. That's psychological manipulation, even when it's carried out automatically. This is bad, even for Facebook."
The universities are distancing themselves from the research. Cornell released a statement on Monday explaining the academics were not involved in collecting the data, only analysing it.
The research paper was edited by Princeton University's Susan Fiske. The psychology professor told The Atlantic the study was technically ok but she is "a little creeped out too".
"It's ethically okay from the regulations perspective, but ethics are kind of social decisions. There's not an absolute answer. And so the level of outrage that appears to be happening suggests that maybe it shouldn't have been done...I'm still thinking about it and I'm a little creeped out, too."
Kramer wrote directly to this concern in his post. He explains they made minimal changes, simply "deprioritising" a small amount of content if it included an emotional word for a small group of people for only a week.
"At the end of the day, the actual impact on people in the experiment was the minimal amount to statistically detect it - the result was that people produced an average of one fewer emotional word, per thousand words, over the following week," Kramer wrote.
Kramer finishes the post explaining researchers at Facebook are working on how to improve their internal review practices, adding the study was created several years ago and arguing they've "come a long way since then".
- SMH
