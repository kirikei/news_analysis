Facebook shouldn't experiment with users' feeds - SFGate
It's not enough for Facebook to control our personal data and our rights to the things we post - now the company wants to control our emotions, too?
The news that Facebook's data scientists skewed the content of what around 700,000 Facebook users saw on their news feeds in order to observe changes in their emotions is deeply upsetting.
We learned about this because of a new study published in the scientific journal Proceedings of the National Academy of Sciences, not because Facebook explained to users that they were being manipulated. That failure to inform users is what makes this experiment so insidious and so wrong.
For one week in January 2012, hundreds of thousands of Facebook users saw something different when they logged on to the site. Some of them were shown more posts and content containing happy and positive words. Some of them were shown the exact opposite: more posts with sad or negative language.
The positive or negative slant was found to be reflected in the users' posts. "We show ... that emotional states can be transferred to others via emotional contagion, leading people to experience the same emotions without their awareness," reads the summary of the finding as described in the paper.
The experiment is almost certainly legal, something that Facebook's spokespeople have noted. In the fine print, Facebook says that user information can be used for internal operations, including research. And it's certainly worth noting that Facebook changes its feed algorithms all the time.
But legal is one thing, and ethical is another.
Many, if not most, research institutions have ethics guidelines that require the informed consent of subjects. If an experiment requires deception, those guidelines often call for subjects to be informed and debriefed afterward. That's not just good manners, it's an important part of the social contract that these institutions have with the public.
With its incredible base of users and the incredible amount of data it has about them, Facebook is an institution. That means it has to start acting like one.
There's some evidence that some people within Facebook understand this. On Sunday afternoon, Adam D.I. Kramer (a Facebook employee and a study author) showed some remorse: "In hindsight, the research benefits of the paper may not have justified all of this anxiety."
That's putting it mildly: This is a public relations nightmare for the company.
Instead of altering users' feeds, we urge Facebook to alter its own fine print about how it's going to use our data.
