Facebook's Emotion Study Was All About Keeping Us Addicted To Facebook - Forbes
Facebook made some errors of judgment in the way it handled its now-infamous, behavioral experiment on users. It only added "research" to its Terms of Service policy four months after it started the study. Then it tinkered with the news feeds of 689,003 people to alter the mood of their own posts, and didn't tell them.
Another goof may have been to publish its findings in an academic journal. While that's great for transparency's sake, the resulting publicity took our perceptions of Facebook's experiment from simply A/B testing a product, to that of mad scientist pulling the strings on unwitting users "to see what happens."
That the most powerful communications utility in the world could manipulate any of its 1.3 billion active users' behavior "for science" is frightening because of what we know about unintended consequences, and the spotty history of psychological experiments. It includes people doing bad things in prison uniforms and hurting others with electric shocks. When Facebook's experiment was billed as a "scientific" one, the idea of clashing corporate interests with academic research suddenly sat very uncomfortably with users and the press.
It got us thinking about Facebook's unprecedented capacity to influence politics. Another experiment in 2012 saw it persuade 60,000 extra people to vote. If those users skewed younger, wouldn't that indirectly benefit left-wing parties? The WSJ's Christopher Mims suggested Mark Zuckerberg might one day be tempted to flex his political ambitions as he approaches the ripe old age of influence-hungry media mogul, and none of us might ever know about it.
Yet ultimately, politics and science are asides. Right now Facebook's power to influence has just one objective: keeping us all on Facebook.
When I asked Michael Brooks, a respected science writer who's covered the placebo effect and research into human free will, about Facebook's study, he pointed not to scientific endeavors but Facebook's bottom line:
Few outside of Facebook truly understand the secret sauce that filters our news feeds. You may have 1,000 friends on the site, but only see a few of the posts they're putting up each day. What's deciding that summary? Facebook says it's showing people "the content they will find most relevant and engaging" -- but those two terms are starting to mean the same thing. We now know that on Facebook, relevance doesn't just have to mean proximity in age or geography, but emotional resonance too. It means that which will keep you coming back to Facebook.
Note that Facebook's data scientists observed a "withdrawal effect" in their Jan 2012 study. When people saw fewer emotional posts, they were "less expressive overall on the following days." They were using the site less.
Fast forward to 2013 when Facebook admitted on a quarterly call with investors that it had seen a drop in teenage users. It would be crazy to think that by this time, Facebook hadn't at least considered using what it had learned from the Jan 2012 study to keep emotionally-driven teens in the Facebook universe.
Facebook's data scientists might have published their findings in an academic journal, but they were ultimately employed to help Facebook understand how to keep its news feeds "relevant and engaging."
That's why in spite of everything we've heard about Facebook overstepping ethical lines on consent and experimentation, we'll continue going back to the site again and again.
