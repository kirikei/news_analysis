Did Facebook overstep its bounds? | Arab News - Saudi Arabia News, Middle East News, Opinion, Economy and more.
Did Facebook overstep its bounds when it ran a secret psychological experiment on a fraction of its users two years ago? That's the question at the heart of an Internet firestorm of the past few days. The consensus is that Facebook probably did something wrong. But what, exactly?
If you're just coming to the story: For a week in 2012, Facebook took a slice of 689,000 English-language accounts across its user base. Then for a random portion of those users, it tweaked the news feed in different ways to change what they saw. For instance, some news feeds were made to be "happier" when Facebook made negative-sounding posts less likely to appear. Other news feeds were made "sadder" when Facebook reduced the incidence of positive-sounding posts.
The apparent goal was to find out whether emotions were contagious on Facebook -- whether happy (or sad) news feeds made users more likely to write happier posts (or sadder posts) themselves. The results were enlightening: The researchers found evidence to suggest that "emotional contagion" exists.
To understand why this has sparked such a vigorous debate, let's unpack some of the charges being lobbed at the social network.
It used people's data for an academic study. The test involved a vast number of accounts. There was no opportunity for those people to consent to the specific study being conducted, despite the fact that by using Facebook, they had technically given Facebook their broad permission to use data for "internal operations, including troubleshooting, data analysis, testing, research and service improvement."
It manipulated people's news feeds to make them happy or sad. This isn't quite right. Facebook wasn't trying to see if it could make people sad just because it could. Rather, it was testing a legitimate hypothesis amid a wider body of academic literature about what Facebook may be doing to us emotionally, culturally and socially. If there's one thing that's problematic about Facebook's methodology, it's probably that outside researchers can't replicate the study to test Facebook's results.
The study made it past an institutional review board. How? Facebook got the approval of an IRB, which are panels designed to assess the ethics of human-subject research. The IRB looked at the results of Facebook's data analysis and gave it the green light, but evidently didn't consider how Facebook acquired the data in the first place. Was that an ethical lapse?
If Facebook were an arm of the government or a federally funded academic institution, then yes. Research conducted in those environments on human subjects requires an IRB's approval. But as a private entity, Facebook isn't legally bound by those requirements.
It's creepy. Beyond all the aforementioned criticism, there's still something deeply unsettling about Facebook's experiment. I suspect the episode says more about us and our growing relationship with data as opposed to anything unique to Facebook's actions. We're just beginning to grasp how pervasive (and potentially invasive) companies can be when they're armed with data.
