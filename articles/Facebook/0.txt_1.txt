UK opens probe into Facebook's psych experiment | Nation & World | The Seattle Times
LONDON --
British regulators are investigating revelations that Facebook treated hordes of its users like laboratory rats in an experiment probing into their emotions.
The Information Commissioner's Office said Wednesday that it wants to learn more about the circumstances underlying a 2-year-old study carried out by two U.S. universities and the world's largest social network.
The inquiry is being coordinated with authorities in Ireland, where Facebook has headquarters for its European operations, as well as with French regulators.
This is just the latest in a string of incidents that have raised questions about whether the privacy rights of Facebook's nearly 1.3 billion users are being trampled by the company's drive to dissect data and promote behavior that could help sell more online advertising.
In this case, Facebook allowed researchers to manipulate the content that appeared in the main section, or "news feed," of about 700,000 randomly selected users during a single week in January 2012. The data-scientists were trying to collect evidence to prove their thesis that people's moods could spread like an "emotional contagion" depending on the tenor of the content that they were reading.
The study concluded that people were more likely to post negative updates about their lives after the volume of positive information appearing in their Facebook feeds had been purposefully reduced by the researchers. The opposite reaction occurred when the number of negative posts appeared in people's news feeds.
None of the participants in the Facebook experiments were explicitly asked for their permission, though the social network's terms of use appears to allow for the company to manipulate what appears in users' news feeds however it sees fits.
Facebook's data-use policy says the Menlo Park, California, company can deploy user information for "internal operations, including troubleshooting, data analysis, testing, research and service improvement."
The reaction to the study itself provided evidence of how quickly an emotional contagion can spread online. The research was released a month ago, but it didn't provoke a backlash until the past few days after other social media sites and essays in The New York Times and The Atlantic raised red flags about the ethics of Facebook's experiment.
As it has done in several past breaches of privacy etiquette, Facebook is now trying to make amends.
Sheryl Sandberg, Facebook's chief operating officer, told television network NDTV in India that "we clearly communicated really badly about this and that we really regret." Later she added: "Facebook has apologized and certainly we never want to do anything that upsets users."
The words of contrition sounded hollow to Jeff Chester, executive director of the Center for Digital Democracy, a privacy-rights group. He points to Facebook job openings looking for researchers specializing in data mining and analysis as evidence that the company still has every intention of digging deeper into its users' psyches and preferences.
"They are engaged in secret surveillance of its users to figure out how to make more money for their advertisers," Chester said.
Whatever Facebook has been doing has been paying off for the company and its shareholders. Facebook's revenue last year rose 55 percent to $7.9 billion and its stock has nearly tripled in value during the past year.
The concern over Facebook's experiment comes amid interest in Europe about beefing up data-protection rules. The European Court of Justice last month ruled that Google must respond to users' requests seeking to remove links to personal information.
Suzy Moat, a Warwick Business School assistant professor of behavioral science, said businesses regularly do studies on how to influence behavior. She cited the example of Facebook and Amazon experimenting with showing different groups of people slightly different versions of their websites to see if one is better than another at getting customers to buy products.
"On the other hand, it's extremely understandable that many people are upset that their behavior may have been manipulated for purely scientific purposes without their consent," Moat said. "In particular, Facebook's user base is so wide that everyone wonders if they were in the experiment."
___
Liedtke reported from San Francisco. Mae Anderson in New York contributed to this report.

Facebook's experiment with emotional contagion - Livemint
For a period of time in early 2012, Facebook's data scientists interfered with the news feeds of more 689,000 unsuspecting users.
Facebook	 is in the news again. No, not for another acquisition or app launch, but for the "behavioural experiment" the social network giant embarked on sometime back. "This was part of ongoing research companies do to test different products, and that was what it was; it was poorly communicated," Facebook's chief operating officer
Sheryl Sandberg	, said.
Users are angry, and the authorities in Britain, Ireland and France are investigating the legality of the experiment.
The experiment
For a period of time in early 2012, Facebook's data scientists interfered with the news feeds of more than 689,000 unsuspecting users. This was part of a study, Experimental Evidence Of Massive-scale Emotional Contagion Through Social Networks, by Facebook, Cornell University and the University of California in the US--the idea was to conduct an "experiment on Facebook to show that emotional states can be transferred to others via , leading people to experience the same emotions without their awareness". Some of these users were shown more negative content than usual, while the news feed of the rest was filled with positive updates and feeds. Facebook wanted to analyse the behavioural changes through posts users might put up--did those positive or negative updates alter their mindset, at least while communicating on Facebook?
Users weren't told
Whenever a situation like this arises, the organization at the receiving end usually pulls out its Terms and Conditions (T&C) book, and points to some clause written in complicated language, which most users may not have bothered to read. In this case that too hasn't worked out well for Facebook. Reports suggest that four months after the experiment began, the word research was added to the T&C, to justify any such activities using user data: "For internal operations, including troubleshooting, data analysis, testing, research and service improvement".
None of the users were asked whether they wished to participate in what is essentially a psychological study.
Did they really apologize?
We already know that Sandberg says the study was "poorly communicated". This seems the kind of apology that doesn't really apologize for the experiment, but for the fact that users were annoyed by this manipulation. At least Facebook has been forthcoming about the entire episode.
Why did this experiment happen in the first place?
In a public Facebook post on 29 June, this is what Facebook data researcher Adam Kramer had to say: "And at the end of the day, the actual impact on people in the experiment was the minimal amount to statistically detect it--the result was that people produced an average of one fewer emotional word, per thousand words, over the following week. The goal of all of our research at Facebook is to learn how to provide a better service. Having written and designed this experiment myself, I can tell you that our goal was never to upset anyone. I can understand why some people have concerns about it.... In hindsight, the research benefits of the paper may not have justified all of this anxiety."
In the Cornell Chronicle, published on 10 June, Cornell University had already outlined its vision on the research: "Facebook, with more than 1.3 billion users of every emotive disposition, and its news feed feature--in which a constantly tweaked, Facebook-controlled ranking algorithm regularly filters posts, stories and activities enjoyed by friends--proved an ideal place to start." It went on to clarify that none of the researchers working on this experiment ever saw the actual content on any of the posts--they monitored positive and negative keywords and their occurrence. Some numbers have been given for this--more than three million posts were analysed, with a total of 122 million words. About four million of those words were "positive" and 1.8 million were "negative".
Is the US government spying on people?
Well, there seems to be some confusion. The same Chronicle article says, "An earlier version of this story reported that the study was funded in part by the James S McDonnell Foundation and the Army Research Office. In fact, the study received no external funding."
But government involvement may not be as far-fetched as it seems to most readers. It has been documented that the US government ran a "Cuban Twitter" account between 2009-12, with the aim of spreading political messages in a country where Web access for the general public was limited. Documents that emerged when Edward Snowden leaked classified documents on US spying programmes have shown how governments are "using online techniques to make something happen in the real or cyber world".
The best response
While people were voicing their displeasure in different ways, Erin Kissane (@akissane), the director of content @OpenNews tweeted: "Get off Facebook. Get your family off Facebook. If you work there, quit. They're f****ng awful."
