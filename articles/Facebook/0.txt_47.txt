UK opens probe into Facebook's psych experiment | Nation & World | The Seattle Times
LONDON --
British regulators are investigating revelations that Facebook treated hordes of its users like laboratory rats in an experiment probing into their emotions.
The Information Commissioner's Office said Wednesday that it wants to learn more about the circumstances underlying a 2-year-old study carried out by two U.S. universities and the world's largest social network.
The inquiry is being coordinated with authorities in Ireland, where Facebook has headquarters for its European operations, as well as with French regulators.
This is just the latest in a string of incidents that have raised questions about whether the privacy rights of Facebook's nearly 1.3 billion users are being trampled by the company's drive to dissect data and promote behavior that could help sell more online advertising.
In this case, Facebook allowed researchers to manipulate the content that appeared in the main section, or "news feed," of about 700,000 randomly selected users during a single week in January 2012. The data-scientists were trying to collect evidence to prove their thesis that people's moods could spread like an "emotional contagion" depending on the tenor of the content that they were reading.
The study concluded that people were more likely to post negative updates about their lives after the volume of positive information appearing in their Facebook feeds had been purposefully reduced by the researchers. The opposite reaction occurred when the number of negative posts appeared in people's news feeds.
None of the participants in the Facebook experiments were explicitly asked for their permission, though the social network's terms of use appears to allow for the company to manipulate what appears in users' news feeds however it sees fits.
Facebook's data-use policy says the Menlo Park, California, company can deploy user information for "internal operations, including troubleshooting, data analysis, testing, research and service improvement."
The reaction to the study itself provided evidence of how quickly an emotional contagion can spread online. The research was released a month ago, but it didn't provoke a backlash until the past few days after other social media sites and essays in The New York Times and The Atlantic raised red flags about the ethics of Facebook's experiment.
As it has done in several past breaches of privacy etiquette, Facebook is now trying to make amends.
Sheryl Sandberg, Facebook's chief operating officer, told television network NDTV in India that "we clearly communicated really badly about this and that we really regret." Later she added: "Facebook has apologized and certainly we never want to do anything that upsets users."
The words of contrition sounded hollow to Jeff Chester, executive director of the Center for Digital Democracy, a privacy-rights group. He points to Facebook job openings looking for researchers specializing in data mining and analysis as evidence that the company still has every intention of digging deeper into its users' psyches and preferences.
"They are engaged in secret surveillance of its users to figure out how to make more money for their advertisers," Chester said.
Whatever Facebook has been doing has been paying off for the company and its shareholders. Facebook's revenue last year rose 55 percent to $7.9 billion and its stock has nearly tripled in value during the past year.
The concern over Facebook's experiment comes amid interest in Europe about beefing up data-protection rules. The European Court of Justice last month ruled that Google must respond to users' requests seeking to remove links to personal information.
Suzy Moat, a Warwick Business School assistant professor of behavioral science, said businesses regularly do studies on how to influence behavior. She cited the example of Facebook and Amazon experimenting with showing different groups of people slightly different versions of their websites to see if one is better than another at getting customers to buy products.
"On the other hand, it's extremely understandable that many people are upset that their behavior may have been manipulated for purely scientific purposes without their consent," Moat said. "In particular, Facebook's user base is so wide that everyone wonders if they were in the experiment."
___
Liedtke reported from San Francisco. Mae Anderson in New York contributed to this report.

The Opinion Pages | Op-Ed Contributor
Should Facebook Manipulate Users?
Jaron Lanier on Lack of Transparency in Facebook Study
By JARON LANIERJUNE 30, 2014
Continue reading the main story Share This Page
SHOULD we worry that technology companies can secretly influence our emotions? Apparently so.
A study recently published by researchers at the University of California, San Francisco, Cornell and Facebook suggests that social networks can manipulate the emotions of their users by tweaking what is allowed into a user's news feed. The study, published in the Proceedings of the National Academy of Sciences, changed the news feeds delivered to almost 700,000 people for a week without getting their consent to be studied. Some got feeds with more sad news, others received more happy news.
The researchers were studying claims that Facebook could make us feel unhappy by creating unrealistic expectations of how good life should be. But it turned out that some subjects were depressed when the good news in their feed was suppressed. Individuals were not asked to report on how they felt; instead, their writing was analyzed for vocabulary choices that were thought to indicate mood.
The researchers claim that they have proved that "emotional states can be transferred to others via emotional contagion, leading people to experience the same emotions without their awareness." The effect was slight, but imposed on a very large population, so it's possible the effects were consequential to some people. The paper itself states its claims rather boldly, but one of the authors, Adam D. I. Kramer of Facebook, responding to intense criticism that it was wrong to study users without their permission, has since emphasized how tiny the effects were. But however the results might be interpreted now, they couldn't have been known in advance.
The manipulation of emotion is no small thing. An estimated 60 percent of suicides are preceded by a mood disorder. Even mild depression has been shown to increase the risk of heart failure by 5 percent; moderate to severe depression increases it by 40 percent.
Research with human subjects is generally governed by strict ethical standards, including the informed consent of the people who are studied. Facebook's generic click-through agreement, which almost no one reads and which doesn't mention this kind of experimentation, was the only form of consent cited in the paper. The subjects in the study still, to this day, have not been informed that they were in the study. If there had been federal funding, such a complacent notion of informed consent would probably have been considered a crime. Subjects would most likely have been screened so that those at special risk would be excluded or handled with extra care.
This is only one early publication about a whole new frontier in the manipulation of people, and Facebook shouldn't be singled out as a villain. All researchers, whether at universities or technology companies, need to focus more on the ethics of how we learn to improve our work.
To promote the relevance of their study, the researchers noted that emotion was relevant to human health, and yet the study didn't measure any potential health effects of the controlled manipulation of emotions.
It is unimaginable that a pharmaceutical firm would be allowed to randomly, secretly sneak an experimental drug, no matter how mild, into the drinks of hundreds of thousands of people, just to see what happens, without ever telling those people. Imagine a pharmaceutical researcher saying, "I was only looking at a narrow research question, so I don't know if my drug harmed anyone, and I haven't bothered to find out." Unfortunately, this seems to be an acceptable attitude when it comes to experimenting with people over social networks. It needs to change.
Our laws require that cars be recalled and fixed even if a defect would be likely to injure only a very small number of people. In this case, we're talking about a study that was actually intended to cause a negative effect in many people, and one open question is how destructive it was in the worst instances that might have occurred.
All of us engaged in research over networks must commit to finding a way to modernize the process of informed consent. Instead of lowering our standards to the level of unread click-through agreements, let's raise the standards for everyone.
Now that we know that a social network proprietor can engineer emotions for the multitudes to a slight degree, we need to consider that further research on amplifying that capacity might take place. Stealth emotional manipulation could be channeled to sell things (you suddenly find that you feel better after buying from a particular store, for instance), but it might also be used to exert influence in a multitude of other ways. Research has also shown that voting behavior can be influenced by undetectable social network maneuvering, for example.
The principle of informed consent in the age of social networking can't be limited to individuals who are studied; the public has every right to be informed of otherwise undetectable commercial or political practices that are made possible by the results of research into high-tech manipulation, and to choose whether to give consent.
My guess is that the public would choose to outlaw using our communication tools as conduits for secret, algorithmic manipulations of our emotions.
Let us choose to live in a society of true hearts, not calculated ones.
Jaron Lanier is the author of "Who Owns the Future," and is an interdisciplinary scientist at Microsoft Research.
A version of this op-ed appears in print on July 1, 2014, on page A21 of the New York edition with the headline: Should Facebook Manipulate Users?. Order Reprints|Today's Paper|Subscribe
