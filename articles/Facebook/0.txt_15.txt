UK opens probe into Facebook's psych experiment | Nation & World | The Seattle Times
LONDON --
British regulators are investigating revelations that Facebook treated hordes of its users like laboratory rats in an experiment probing into their emotions.
The Information Commissioner's Office said Wednesday that it wants to learn more about the circumstances underlying a 2-year-old study carried out by two U.S. universities and the world's largest social network.
The inquiry is being coordinated with authorities in Ireland, where Facebook has headquarters for its European operations, as well as with French regulators.
This is just the latest in a string of incidents that have raised questions about whether the privacy rights of Facebook's nearly 1.3 billion users are being trampled by the company's drive to dissect data and promote behavior that could help sell more online advertising.
In this case, Facebook allowed researchers to manipulate the content that appeared in the main section, or "news feed," of about 700,000 randomly selected users during a single week in January 2012. The data-scientists were trying to collect evidence to prove their thesis that people's moods could spread like an "emotional contagion" depending on the tenor of the content that they were reading.
The study concluded that people were more likely to post negative updates about their lives after the volume of positive information appearing in their Facebook feeds had been purposefully reduced by the researchers. The opposite reaction occurred when the number of negative posts appeared in people's news feeds.
None of the participants in the Facebook experiments were explicitly asked for their permission, though the social network's terms of use appears to allow for the company to manipulate what appears in users' news feeds however it sees fits.
Facebook's data-use policy says the Menlo Park, California, company can deploy user information for "internal operations, including troubleshooting, data analysis, testing, research and service improvement."
The reaction to the study itself provided evidence of how quickly an emotional contagion can spread online. The research was released a month ago, but it didn't provoke a backlash until the past few days after other social media sites and essays in The New York Times and The Atlantic raised red flags about the ethics of Facebook's experiment.
As it has done in several past breaches of privacy etiquette, Facebook is now trying to make amends.
Sheryl Sandberg, Facebook's chief operating officer, told television network NDTV in India that "we clearly communicated really badly about this and that we really regret." Later she added: "Facebook has apologized and certainly we never want to do anything that upsets users."
The words of contrition sounded hollow to Jeff Chester, executive director of the Center for Digital Democracy, a privacy-rights group. He points to Facebook job openings looking for researchers specializing in data mining and analysis as evidence that the company still has every intention of digging deeper into its users' psyches and preferences.
"They are engaged in secret surveillance of its users to figure out how to make more money for their advertisers," Chester said.
Whatever Facebook has been doing has been paying off for the company and its shareholders. Facebook's revenue last year rose 55 percent to $7.9 billion and its stock has nearly tripled in value during the past year.
The concern over Facebook's experiment comes amid interest in Europe about beefing up data-protection rules. The European Court of Justice last month ruled that Google must respond to users' requests seeking to remove links to personal information.
Suzy Moat, a Warwick Business School assistant professor of behavioral science, said businesses regularly do studies on how to influence behavior. She cited the example of Facebook and Amazon experimenting with showing different groups of people slightly different versions of their websites to see if one is better than another at getting customers to buy products.
"On the other hand, it's extremely understandable that many people are upset that their behavior may have been manipulated for purely scientific purposes without their consent," Moat said. "In particular, Facebook's user base is so wide that everyone wonders if they were in the experiment."
___
Liedtke reported from San Francisco. Mae Anderson in New York contributed to this report.

We are the product that Facebook has been testing | GulfNews.com
There is an argument that the latest Facebook scandal is a lot of fuss about nothing. A week-long psychological experiment on 690,000 users in 2012, that did no damage and had a barely noticeable effect, hardly registers on the scale of research abuses over the years.
Nor does hiding a few positive and negative posts from a small fraction of Facebook's 1.3 billion users in order to monitor "emotional contagion" compare with the harm to which other companies sometimes expose customers. Drivers of faulty cars and consumers of processed food stuffed with fat and salt take far bigger risks with their health. Facebook has made no secret of the fact that its news feed is a manipulated version of reality. It selects the posts and links to display prominently what it has found through testing are the most likely to interest users, and encourage them to return and post themselves. These tests are not sinister experiments; they are product development.
All true, but there is one big difference: We are the product that Facebook has been testing. Perhaps we should grow up and accept that this is how the world works when we use an advertising-funded social network stuffed with details of our lives and those of family and friends. But if we find it creepy, that is what the experiment proves. With this blunder, Facebook's data scientists have drawn to attention two things of which most of us may have been vaguely aware, but have not pondered very hard.
First, Facebook does not feel the need to ask for permission before carrying out its tests. Its terms of use, the screed of text that most of us scroll through rapidly to click "yes" at the end and move on, includes a reference to using data to "improve" the product. Since 2012, the word "research" has also been included. That is it. Its research paper, published in the Proceedings of the National Academy of Science of the United States of America, claims the "informed consent" of Facebook users, which is blatantly false. Making all users agree to a catch-all list that does not provide a clear, specific description of a study fails the 1979 consent test for US academic research .
Edward Felten, a professor of computer science and public affairs at Princeton University, describes its terms of use as "a legal fiction of consent" in academic terms. Even commercially, Facebook is in a privileged position. Many companies such as Unilever and Procter & Gamble carry out psychological studies, but they recruit subjects: You are not made to sign a research waiver to buy an ice-cream.
As well as being big, Facebook holds more intimate information about its users than other internet companies. The algorithm that controls the news feed is similar to that which powers Google's search rankings: Both list material by how relevant it is, using an array of data. Google's display is also influenced by users' interests and responses.
Second, Facebook wields incredible power over the behaviour of users. This is partly the result of its sheer size. Another research study by Facebook's scientists, on how information is spread by networks of friends, notes that "our sample consists of approximately 253 million" users. In other words, they experimented on the equivalent of four times the population of France.
The emotional contagion paper concludes that "given the massive scale of social networks such as Facebook, small effects can have large aggregated consequences". This experiment with altering users' news feeds only had a tiny impact on their behaviour but even it "would have corresponded to hundreds of thousands of emotion expressions in status updates per day".
Google, however, analyses material from across the web, whereas Facebook focuses its judgements on personal material. An algorithm that selects from thousands of links about, say, Buckingham Palace feels like a service; one that weeds out the posts of friends and family feels like a moral guardian.
Facebook has demonstrated that it can alter behaviour -- that is what its experiments are about. "We regularly run tests to work out how to make the experience better. Through testing, we have found that when people see more text status updates on Facebook, they write more status updates themselves," a manager wrote in January.
Occasionally, it uses its powers for a specific purpose, as in 2012 when founder Mark Zuckerberg nudged Facebook users towards being organ donors by allowing existing donors to display that status and to encourage friends to follow them (it had a significant take-up). Mostly, it tinkers with the algorithm with the sole purpose of stimulating user growth and activity.
Zuckerberg and other Facebook executives share the Panglossian philosophy that what is good for its users -- sharing material with their friends -- is good for Facebook, and whatever promotes it is beneficial for everyone. "The goal of news feed is to deliver the right content to the right people at the right time" is the service's oft-repeated, deceptively simple nostrum. This accounts for the innocently surprised tone of the apology offered last week by Adam Kramer, the Facebook data scientist who designed the controversial emotional contagion study. "The goal of all of our research at Facebook is to learn how to provide a better service ... I can tell you that our goal was never to upset anyone," he wrote.
Naturally, it was not. But the US academic research guidelines were drawn up long ago to stop people who believed they had the greater good in mind behaving as they thought was best. It is time for Facebook to read them.
